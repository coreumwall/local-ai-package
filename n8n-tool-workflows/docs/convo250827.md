# Research Assistant Conversation & Strategy

### Segment 2: Research Assistant App Specification

- **User (01:29):**
  > I'd like you to help me spec out a research assistant app. I have a group of apps that are connected in this. It’s called a local AI package. It consists of OpenWebUI, n8n, LangFuse, FlowWise, a Supabase-based database, some type of search app (SEAR or a private search capability). I’d like to ensure all these are interwoven to serve a common purpose – doing directed research and building a database, visualized as a node graph (possibly in 3D or with time as an extra dimension) to help identify niche opportunities.
- **ChatGPT:**
  > In plain terms, you want a research assistant setup that ties together all these tools so they function cohesively. The goal is to perform targeted research, build a visual database (node graph), and discover niche opportunities.



### Segment 3: System Integration and Containerization

- **User:**
  > They don't have to be a single monolithic system; they can run in separate Docker containers, connected via webhooks (e.g., linking OpenWebUI and n8n). Containers are preferred locally for flexibility as well as scalability (cloud-based if needed).
- **ChatGPT:**
  > Right, keeping them containerized is practical. Separate containers with communication via webhooks allow you to switch easily and scale when necessary.

---

### Segment 4: Maintaining Local Privacy

- **User:**
  > I like to keep the research local to avoid prying eyes.
- **ChatGPT:**
  > Understood. Containerization and local-first operations ensure privacy while maintaining flexibility.

---

### Segment 5: Expanding the System Beyond Research

- **User (01:12):**
  > Please suggest ways to connect these tools into a system that is more than the sum of its parts. Starting with a node graph to visualize the database and then linking tools like Flowise and n8n. Eventually, I’d like the system to synthesize research, generate code snippets, and integrate with external MCP servers, MATLAB, PyAnsys, etc.
- **ChatGPT:**
  > Start with a foundation that includes a node graph layer (using, for example, Neo4j). Use webhooks/APIs to allow Flowise and n8n to communicate seamlessly. This foundation will already support research and later synthesis, code generation, and external integrations.

---

### Segment 6: Specific Workflow Ideas

- **User:**
  > What about specialized workflows? One possibility is assigning dedicated agents – for instance, one agent focusing on patent searches (using tools like Apify or patent databases) and another accessing other research databases via an Appify MCP server.
- **ChatGPT:**
  > A solid idea: set up specialized agents for specific tasks. Use Flowise to orchestrate and n8n to automate data flow between agents. In this way, every piece works on its strength, collectively providing enhanced capability.

---

### Segment 7: Embedding Technical Drawings and Overlaid Databases

- **User:**
  > I’m interested in an embedding solution that handles not only text but also technical drawings from PDFs. I recall a demo (possibly a product called Docling) that could extract dimensions from technical schematics. Also, can databases be overlaid via a node graph even if their data comes from different embedders?
- **ChatGPT:**
  > Yes — you need an embedding solution that works with both text and technical drawings. Open-source options to explore include Docling (or a similar project) and LayoutParser. Moreover, when data is vectorized consistently, a node graph can overlay multiple databases, clustering related topics together despite differences in embedder details.

---

### Segment 8: End-to-End Plan Request

- **User:**
  > Based on our conversation, could you put together a document that lays out a general strategy to connect these tools, the database structure, and insights into the internal database organization (including relevant tools like Node.js, etc.)?
- **ChatGPT:**
  > (After a thoughtful pause, ChatGPT details a comprehensive plan outlining components, data flow, database schema, and visualization options.)


## 2. Detailed Research Assistant Architecture Strategy

This section outlines a comprehensive high-level strategy for your local research assistant stack.

### 2.1 Overview

The integrated system involves:

- **Interface & Operator Console:**
  - *OpenWebUI* – Acts as the operator console and tool host (using MCP/OpenAPI tools).
- **Workflow Orchestration:**
  - *Flowise* – Visual orchestration and management of multi-agent chains.
  - *n8n* – Scheduler, ETL, and event glue for tasks like API calls and webhook triggers.
- **Data Storage & Search:**
  - *Supabase (Postgres + pgvector)* – Primary document store, vector store, and metadata repository.
  - *SearXNG* – Private meta-search layer.
- **Graph Visualization:**
  - *Graph DB (Neo4j or Memgraph)* – For constructing a visual node graph with temporal dimensions.
- **Monitoring & Evaluation:**
  - *LangFuse* – For tracking agent performance and system metrics.

---

### 2.2 Data Flow and Pipeline

1. **Ingestion (n8n):**
   - Crawl web content, patent APIs, and user files.
   - Use triggers (e.g., cron jobs, webhooks) to initiate data collection.

2. **Parsing & Structuring:**
   - Tools such as *Docling*, *Nougat*, or *Donut* process PDFs (including technical drawings) into normalized JSON/Markdown and images.

3. **Indexing & Embedding:**
   - **Chunking:** Break documents into sections/chunks for indexing.
   - **Embedding:** Generate and store vector embeddings of text (and image features) in Supabase.

4. **Graph Formation:**
   - Extract entities, claims, and relationships from indexed data.
   - Upsert these relationships into a graph database (e.g., Neo4j), with time-aware edges.

5. **Utilization:**
   - Use *OpenWebUI*, along with a custom frontend, to prompt for research queries and visualize results as an interactive node graph.

6. **Monitoring:**
   - Employ *LangFuse* for tracing, evaluations, and performance metrics.

---

### 2.3 Database Schema (Supabase/Postgres + pgvector)

#### Documents and Sources

```sql
create table research.document (
  id uuid primary key default gen_random_uuid(),
  uri text,                          -- file path or URL
  source_kind text,                  -- 'pdf','web','patent','note'
  title text,
  authors text[],
  published_at timestamptz,
  ingested_at timestamptz default now(),
  content_hash text unique
);
```

#### Document Chunks

```sql
create table research.chunk (
  id uuid primary key default gen_random_uuid(),
  document_id uuid references research.document(id) on delete cascade,
  chunk_ix int,
  text text,
  section jsonb,         -- e.g., ["Introduction", "Fig 2 caption"]
  page_num int,
  bbox int[]             -- [x1,y1,x2,y2] from PDF
);
```

#### Vector Embeddings (using pgvector extension)

```sql
create extension if not exists vector;
create table research.embedding (
  chunk_id uuid primary key references research.chunk(id) on delete cascade,
  model text not null,
  dim int not null,
  embedding vector(1024) not null
);
-- Create HNSW index for fast approximate nearest neighbor search
create index on research.embedding using hnsw (embedding vector_cosine_ops);
```

#### Figures and Their Embeddings

```sql
create table research.figure (
  id uuid primary key default gen_random_uuid(),
  document_id uuid references research.document(id) on delete cascade,
  page_num int,
  kind text,               -- 'diagram','chart','table','photo'
  image_path text,         -- local path to image
  ocr jsonb,               -- OCR tokens with coordinates
  caption text
);

create table research.figure_embedding (
  figure_id uuid primary key references research.figure(id) on delete cascade,
  model text,
  dim int,
  embedding vector(1024)
);
```

#### Claims and Citations

```sql
create table research.claim (
  id uuid primary key default gen_random_uuid(),
  text text not null,
  created_at timestamptz default now(),
  confidence real,
  provenance jsonb         -- e.g., {document_id, chunk_id, offsets}
);

create table research.citation (
  claim_id uuid references research.claim(id) on delete cascade,
  document_id uuid references research.document(id),
  chunk_id uuid references research.chunk(id),
  note text,
  primary key (claim_id, document_id, chunk_id)
);
```

---

### 2.4 Knowledge Graph (3D + Time)

- **Nodes:** Entities (people, organizations, materials), Topics, Documents, Figures, Claims.
- **Edges:** Relationships like MENTIONS, DERIVED_FROM, CITES, SUPPORTS, CONTRADICTS, SIMILAR_TO – with properties such as time validity (`t_valid_from`, `t_valid_to`) and weights.
- **Visualization:** Use tools such as `3d-force-graph` or `react-force-graph-3d` to build an interactive graph, potentially enhanced with time sliders for temporal context.

---

### 2.5 Workflow Orchestration

- **n8n:**
  - Trigger and schedule ingestion tasks (web scraping, API calls).
  - Process and upsert data to Supabase.
  - Trigger workflows across agents via webhooks.

- **Flowise:**
  - Orchestrate multi-agent chains for specialized tasks (e.g., a dedicated patent search agent).
  - Integrate responses and route data between n8n and OpenWebUI.

---

### 2.6 Additional Considerations

- **Embedding Technical Drawings:**
  - Investigate open-source projects like **Docling** and **LayoutParser** for processing PDFs with diagrams.

- **Security & Privacy:**
  - Maintain a local-first setup to restrict external access and ensure privacy.

- **Scalability:**
  - Use Docker containers to run each service independently; migrate to cloud deployment if scaling is needed.

---

## 3. Visualization and Future Expansion

- **Graph Visualization Tools:**
  - Embed interactive 3D graphs using libraries such as `3d-force-graph` or `react-force-graph-3d`.
  - Consider Neo4j Bloom for ad hoc graph exploration without custom code.

- **Monitoring with LangFuse:**
  - Continuously trace agent performance and system evaluations to refine the workflow.

- **Next Steps:**
  - Once the research foundation is established, extend the system to synthesize findings, generate code snippets, and integrate with computational tools (MATLAB, PyAnsys) for advanced data analysis.

---

*This document outlines an end-to-end strategy for building a cohesive, containerized research assistant platform that leverages a mix of traditional databases, vector search, and graph visualization to empower directed research and synthesis.*